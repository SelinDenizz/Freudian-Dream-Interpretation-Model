{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SelinDenizz/Freudian-Dream-Interpretation-Model/blob/main/notebook/data_trainer_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVe5gnA4puSK"
      },
      "outputs": [],
      "source": [
        "import importlib.util\n",
        "import os\n",
        "\n",
        "def is_installed(package_name):\n",
        "    return importlib.util.find_spec(package_name) is not None\n",
        "\n",
        "if not is_installed(\"unsloth\"):\n",
        "\n",
        "    !pip uninstall -y protobuf fsspec torch torchaudio torchvision\n",
        "    !pip install \"protobuf<4.0.0\" fsspec==2025.3.0\n",
        "    !pip install torch==2.6.0+cu124 torchaudio==2.6.0+cu124 torchvision==0.21.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "    !pip install -q unsloth transformers peft datasets gradio pandas tqdm\n",
        "    !pip check\n",
        "\n",
        "    requirements_path = \"/content/drive/MyDrive/freudian_dream_analyzer/requirements.txt\"\n",
        "    !mkdir -p /content/drive/MyDrive/freudian_dream_analyzer\n",
        "    !pip freeze > \"{requirements_path}\"\n",
        "    print(f\"requirements.txt saved at {requirements_path}\")\n",
        "else:\n",
        "    print(\"Environment already installed, skipping setup.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEe-C3_6OMFd"
      },
      "outputs": [],
      "source": [
        "# Runtime & GPU check\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"No GPU available. Enable it from Runtime > Change runtime type.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWqm89x3OMFd"
      },
      "outputs": [],
      "source": [
        "# First, unmount if already mounted\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.flush_and_unmount()\n",
        "    print('Existing drive mount was unmounted')\n",
        "except:\n",
        "    print('No existing drive mount')\n",
        "\n",
        "# Check and clean up the mount point\n",
        "import os\n",
        "import shutil\n",
        "if os.path.exists('/content/drive'):\n",
        "    if os.path.isdir('/content/drive') and os.listdir('/content/drive'):\n",
        "        shutil.rmtree('/content/drive')\n",
        "        print(\"Removed existing /content/drive directory and its contents\")\n",
        "\n",
        "    elif os.path.isfile('/content/drive'):\n",
        "        os.remove('/content/drive')\n",
        "        print(\"Removed existing /content/drive file\")\n",
        "\n",
        "os.makedirs('/content/drive', exist_ok=True)\n",
        "print(\"Created fresh /content/drive directory\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/freudian_dream_analyzer/\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/freudian_dream_analyzer/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Ua1a8pEh-d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import importlib.util\n",
        "import os\n",
        "\n",
        "project_root = \"/content/drive/MyDrive/freudian_dream_analyzer\"\n",
        "sys.path.append(project_root)\n",
        "\n",
        "module_path = os.path.join(project_root, \"script/module/data_trainer.py\")\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"data_trainer\", module_path)\n",
        "data_trainer = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(data_trainer)\n",
        "\n",
        "UnslothTrainer = data_trainer.UnslothTrainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zYMxsWlJqLZ"
      },
      "outputs": [],
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    max_seq_length=2048,\n",
        "    micro_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    lora_r=int(16),\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bf16=True,\n",
        "    tf32=False,\n",
        "    save_steps=100\n",
        ")\n",
        "\n",
        "model_path = trainer.finetune(\n",
        "    jsonl_file=\"/content/drive/MyDrive/freudian_dream_analyzer/data/dream/processed/fine_tuning_format/dreambank_finetune_llama.jsonl\",\n",
        "    output_dir=\"/content/drive/MyDrive/freudian_dream_analyzer/model/unsloth_model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# UPLOAD YOUR CUSTOM MODEL TO GOOGLE COLAB\n",
        "# Run this in a separate cell BEFORE running the main application\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create the model directory\n",
        "model_dir = '/content/model/unsloth_model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "print(f\"‚úÖ Created directory: {model_dir}\")\n",
        "\n",
        "# Upload your model files\n",
        "print(\"\"\"\n",
        "üìÅ UPLOAD YOUR MODEL FILES:\n",
        "\n",
        "From your local computer, you need to upload these files from your 'model/unsloth_model' folder:\n",
        "- adapter_config.json\n",
        "- adapter_model.safetensors\n",
        "- special_tokens_map.json\n",
        "- tokenizer_config.json\n",
        "- tokenizer.json\n",
        "- tokenizer.model\n",
        "- Any other files in your model directory\n",
        "\n",
        "Click 'Choose Files' below and select ALL your model files:\n",
        "\"\"\")\n",
        "\n",
        "# File upload interface\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to the correct directory\n",
        "if uploaded:\n",
        "    print(f\"\\nüìÇ Processing {len(uploaded)} uploaded files:\")\n",
        "    for filename, content in uploaded.items():\n",
        "        filepath = os.path.join(model_dir, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"‚úÖ Saved: {filename} ({len(content)} bytes)\")\n",
        "\n",
        "    print(f\"\\nüéØ Model files uploaded to: {model_dir}\")\n",
        "\n",
        "    # Verify the upload\n",
        "    print(f\"\\nüìã Files in model directory:\")\n",
        "    try:\n",
        "        for file in sorted(os.listdir(model_dir)):\n",
        "            file_path = os.path.join(model_dir, file)\n",
        "            size = os.path.getsize(file_path)\n",
        "            print(f\"  - {file} ({size:,} bytes)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error listing files: {e}\")\n",
        "\n",
        "    # Check for required LoRA files\n",
        "    adapter_config = os.path.join(model_dir, 'adapter_config.json')\n",
        "    adapter_model = os.path.join(model_dir, 'adapter_model.safetensors')\n",
        "\n",
        "    if os.path.exists(adapter_config):\n",
        "        print(f\"\\n‚úÖ Found adapter_config.json - LoRA model detected\")\n",
        "\n",
        "        # Read adapter config to get base model info\n",
        "        try:\n",
        "            import json\n",
        "            with open(adapter_config, 'r') as f:\n",
        "                config = json.load(f)\n",
        "            base_model = config.get('base_model_name_or_path', 'Unknown')\n",
        "            print(f\"üìä Base model: {base_model}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not read adapter config: {e}\")\n",
        "\n",
        "    if os.path.exists(adapter_model):\n",
        "        print(f\"‚úÖ Found adapter_model.safetensors\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è adapter_model.safetensors not found - this might cause issues\")\n",
        "\n",
        "    print(f\"\\nüöÄ Ready! Now run the main application script to use your custom model.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No files uploaded. Please try again.\")\n",
        "\n",
        "# =============================================================================\n",
        "# ALTERNATIVE: Upload via Google Drive (if you have files there)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "üìÅ ALTERNATIVE UPLOAD METHOD - Google Drive:\n",
        "\n",
        "If your model files are in Google Drive, run this instead:\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy your model files from Drive\n",
        "# Update this path to where your model files are stored in Drive\n",
        "source_path = '/content/drive/MyDrive/your_model_folder/unsloth_model'\n",
        "destination_path = '/content/model/unsloth_model'\n",
        "\n",
        "if os.path.exists(source_path):\n",
        "    shutil.copytree(source_path, destination_path, dirs_exist_ok=True)\n",
        "    print(f\"‚úÖ Copied model from Google Drive: {source_path}\")\n",
        "\n",
        "    # List copied files\n",
        "    for file in os.listdir(destination_path):\n",
        "        print(f\"  - {file}\")\n",
        "else:\n",
        "    print(f\"‚ùå Model folder not found in Google Drive: {source_path}\")\n",
        "    print(\"Please update the source_path to match your Drive folder structure\")\n",
        "```\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# VERIFY MODEL STRUCTURE\n",
        "# =============================================================================\n",
        "\n",
        "def verify_model_structure():\n",
        "    \"\"\"Verify if the uploaded model has the correct structure\"\"\"\n",
        "    model_dir = '/content/model/unsloth_model'\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        print(\"‚ùå Model directory doesn't exist yet\")\n",
        "        return False\n",
        "\n",
        "    files = os.listdir(model_dir)\n",
        "    if not files:\n",
        "        print(\"‚ùå Model directory is empty\")\n",
        "        return False\n",
        "\n",
        "    # Check for LoRA adapter files\n",
        "    required_lora_files = ['adapter_config.json']\n",
        "    lora_files_present = [f for f in required_lora_files if f in files]\n",
        "\n",
        "    # Check for full model files\n",
        "    common_model_files = ['config.json', 'pytorch_model.bin', 'model.safetensors']\n",
        "    model_files_present = [f for f in common_model_files if f in files]\n",
        "\n",
        "    if lora_files_present:\n",
        "        print(\"‚úÖ LoRA adapter model detected\")\n",
        "        return True\n",
        "    elif model_files_present:\n",
        "        print(\"‚úÖ Full model detected\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Unknown model structure\")\n",
        "        print(f\"Files found: {files}\")\n",
        "        return True  # Let the app try to load it anyway\n",
        "\n",
        "# Run verification\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL VERIFICATION:\")\n",
        "verify_model_structure()"
      ],
      "metadata": {
        "id": "LGYZx_ojPOYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}