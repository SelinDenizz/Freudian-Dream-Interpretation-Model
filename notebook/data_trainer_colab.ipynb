{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SelinDenizz/Freudian-Dream-Interpretation-Model/blob/main/notebook/data_trainer_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVe5gnA4puSK"
      },
      "outputs": [],
      "source": [
        "import importlib.util\n",
        "import os\n",
        "\n",
        "def is_installed(package_name):\n",
        "    return importlib.util.find_spec(package_name) is not None\n",
        "\n",
        "if not is_installed(\"unsloth\"):\n",
        "\n",
        "    !pip uninstall -y protobuf fsspec torch torchaudio torchvision\n",
        "    !pip install \"protobuf<4.0.0\" fsspec==2025.3.0\n",
        "    !pip install torch==2.6.0+cu124 torchaudio==2.6.0+cu124 torchvision==0.21.0+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "    !pip install -q unsloth transformers peft datasets gradio pandas tqdm\n",
        "    !pip check\n",
        "\n",
        "    requirements_path = \"/content/drive/MyDrive/freudian_dream_analyzer/requirements.txt\"\n",
        "    !mkdir -p /content/drive/MyDrive/freudian_dream_analyzer\n",
        "    !pip freeze > \"{requirements_path}\"\n",
        "    print(f\"requirements.txt saved at {requirements_path}\")\n",
        "else:\n",
        "    print(\"Environment already installed, skipping setup.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEe-C3_6OMFd"
      },
      "outputs": [],
      "source": [
        "# Runtime & GPU check\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "if IN_COLAB:\n",
        "    print(\"Running in Google Colab\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"No GPU available. Enable it from Runtime > Change runtime type.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWqm89x3OMFd"
      },
      "outputs": [],
      "source": [
        "# First, unmount if already mounted\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.flush_and_unmount()\n",
        "    print('Existing drive mount was unmounted')\n",
        "except:\n",
        "    print('No existing drive mount')\n",
        "\n",
        "# Check and clean up the mount point\n",
        "import os\n",
        "import shutil\n",
        "if os.path.exists('/content/drive'):\n",
        "    if os.path.isdir('/content/drive') and os.listdir('/content/drive'):\n",
        "        shutil.rmtree('/content/drive')\n",
        "        print(\"Removed existing /content/drive directory and its contents\")\n",
        "\n",
        "    elif os.path.isfile('/content/drive'):\n",
        "        os.remove('/content/drive')\n",
        "        print(\"Removed existing /content/drive file\")\n",
        "\n",
        "os.makedirs('/content/drive', exist_ok=True)\n",
        "print(\"Created fresh /content/drive directory\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/freudian_dream_analyzer/\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/freudian_dream_analyzer/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Ua1a8pEh-d"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import importlib.util\n",
        "import os\n",
        "\n",
        "project_root = \"/content/drive/MyDrive/freudian_dream_analyzer\"\n",
        "sys.path.append(project_root)\n",
        "\n",
        "module_path = os.path.join(project_root, \"script/module/data_trainer.py\")\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"data_trainer\", module_path)\n",
        "data_trainer = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(data_trainer)\n",
        "\n",
        "UnslothTrainer = data_trainer.UnslothTrainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zYMxsWlJqLZ"
      },
      "outputs": [],
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    max_seq_length=2048,\n",
        "    micro_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    lora_r=int(16),\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bf16=True,\n",
        "    tf32=False,\n",
        "    save_steps=100\n",
        ")\n",
        "\n",
        "model_path = trainer.finetune(\n",
        "    jsonl_file=\"/content/drive/MyDrive/freudian_dream_analyzer/data/dream/processed/fine_tuning_format/dreambank_finetune_llama.jsonl\",\n",
        "    output_dir=\"/content/drive/MyDrive/freudian_dream_analyzer/model/unsloth_model\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# UPLOAD YOUR CUSTOM MODEL TO GOOGLE COLAB\n",
        "# Run this in a separate cell BEFORE running the main application\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create the model directory\n",
        "model_dir = '/content/model/unsloth_model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "print(f\"‚úÖ Created directory: {model_dir}\")\n",
        "\n",
        "# Upload your model files\n",
        "print(\"\"\"\n",
        "üìÅ UPLOAD YOUR MODEL FILES:\n",
        "\n",
        "From your local computer, you need to upload these files from your 'model/unsloth_model' folder:\n",
        "- adapter_config.json\n",
        "- adapter_model.safetensors\n",
        "- special_tokens_map.json\n",
        "- tokenizer_config.json\n",
        "- tokenizer.json\n",
        "- tokenizer.model\n",
        "- Any other files in your model directory\n",
        "\n",
        "Click 'Choose Files' below and select ALL your model files:\n",
        "\"\"\")\n",
        "\n",
        "# File upload interface\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded files to the correct directory\n",
        "if uploaded:\n",
        "    print(f\"\\nüìÇ Processing {len(uploaded)} uploaded files:\")\n",
        "    for filename, content in uploaded.items():\n",
        "        filepath = os.path.join(model_dir, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(content)\n",
        "        print(f\"‚úÖ Saved: {filename} ({len(content)} bytes)\")\n",
        "\n",
        "    print(f\"\\nüéØ Model files uploaded to: {model_dir}\")\n",
        "\n",
        "    # Verify the upload\n",
        "    print(f\"\\nüìã Files in model directory:\")\n",
        "    try:\n",
        "        for file in sorted(os.listdir(model_dir)):\n",
        "            file_path = os.path.join(model_dir, file)\n",
        "            size = os.path.getsize(file_path)\n",
        "            print(f\"  - {file} ({size:,} bytes)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error listing files: {e}\")\n",
        "\n",
        "    # Check for required LoRA files\n",
        "    adapter_config = os.path.join(model_dir, 'adapter_config.json')\n",
        "    adapter_model = os.path.join(model_dir, 'adapter_model.safetensors')\n",
        "\n",
        "    if os.path.exists(adapter_config):\n",
        "        print(f\"\\n‚úÖ Found adapter_config.json - LoRA model detected\")\n",
        "\n",
        "        # Read adapter config to get base model info\n",
        "        try:\n",
        "            import json\n",
        "            with open(adapter_config, 'r') as f:\n",
        "                config = json.load(f)\n",
        "            base_model = config.get('base_model_name_or_path', 'Unknown')\n",
        "            print(f\"üìä Base model: {base_model}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not read adapter config: {e}\")\n",
        "\n",
        "    if os.path.exists(adapter_model):\n",
        "        print(f\"‚úÖ Found adapter_model.safetensors\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è adapter_model.safetensors not found - this might cause issues\")\n",
        "\n",
        "    print(f\"\\nüöÄ Ready! Now run the main application script to use your custom model.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No files uploaded. Please try again.\")\n",
        "\n",
        "# =============================================================================\n",
        "# ALTERNATIVE: Upload via Google Drive (if you have files there)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\"\"\n",
        "üìÅ ALTERNATIVE UPLOAD METHOD - Google Drive:\n",
        "\n",
        "If your model files are in Google Drive, run this instead:\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Copy your model files from Drive\n",
        "# Update this path to where your model files are stored in Drive\n",
        "source_path = '/content/drive/MyDrive/your_model_folder/unsloth_model'\n",
        "destination_path = '/content/model/unsloth_model'\n",
        "\n",
        "if os.path.exists(source_path):\n",
        "    shutil.copytree(source_path, destination_path, dirs_exist_ok=True)\n",
        "    print(f\"‚úÖ Copied model from Google Drive: {source_path}\")\n",
        "\n",
        "    # List copied files\n",
        "    for file in os.listdir(destination_path):\n",
        "        print(f\"  - {file}\")\n",
        "else:\n",
        "    print(f\"‚ùå Model folder not found in Google Drive: {source_path}\")\n",
        "    print(\"Please update the source_path to match your Drive folder structure\")\n",
        "```\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# VERIFY MODEL STRUCTURE\n",
        "# =============================================================================\n",
        "\n",
        "def verify_model_structure():\n",
        "    \"\"\"Verify if the uploaded model has the correct structure\"\"\"\n",
        "    model_dir = '/content/model/unsloth_model'\n",
        "\n",
        "    if not os.path.exists(model_dir):\n",
        "        print(\"‚ùå Model directory doesn't exist yet\")\n",
        "        return False\n",
        "\n",
        "    files = os.listdir(model_dir)\n",
        "    if not files:\n",
        "        print(\"‚ùå Model directory is empty\")\n",
        "        return False\n",
        "\n",
        "    # Check for LoRA adapter files\n",
        "    required_lora_files = ['adapter_config.json']\n",
        "    lora_files_present = [f for f in required_lora_files if f in files]\n",
        "\n",
        "    # Check for full model files\n",
        "    common_model_files = ['config.json', 'pytorch_model.bin', 'model.safetensors']\n",
        "    model_files_present = [f for f in common_model_files if f in files]\n",
        "\n",
        "    if lora_files_present:\n",
        "        print(\"‚úÖ LoRA adapter model detected\")\n",
        "        return True\n",
        "    elif model_files_present:\n",
        "        print(\"‚úÖ Full model detected\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Unknown model structure\")\n",
        "        print(f\"Files found: {files}\")\n",
        "        return True  # Let the app try to load it anyway\n",
        "\n",
        "# Run verification\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL VERIFICATION:\")\n",
        "verify_model_structure()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LGYZx_ojPOYL",
        "outputId": "f102f101-4436-4006-f20c-528674727b5b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Created directory: /content/model/unsloth_model\n",
            "\n",
            "üìÅ UPLOAD YOUR MODEL FILES:\n",
            "\n",
            "From your local computer, you need to upload these files from your 'model/unsloth_model' folder:\n",
            "- adapter_config.json\n",
            "- adapter_model.safetensors  \n",
            "- special_tokens_map.json\n",
            "- tokenizer_config.json\n",
            "- tokenizer.json\n",
            "- tokenizer.model\n",
            "- Any other files in your model directory\n",
            "\n",
            "Click 'Choose Files' below and select ALL your model files:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-75554518-23d4-4d05-8ab1-c90f1a1045f2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-75554518-23d4-4d05-8ab1-c90f1a1045f2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving adapter_config.json to adapter_config.json\n",
            "Saving adapter_model.safetensors to adapter_model.safetensors\n",
            "Saving special_tokens_map.json to special_tokens_map.json\n",
            "Saving tokenizer.json to tokenizer.json\n",
            "Saving tokenizer.model to tokenizer.model\n",
            "Saving tokenizer_config.json to tokenizer_config.json\n",
            "\n",
            "üìÇ Processing 6 uploaded files:\n",
            "‚úÖ Saved: adapter_config.json (841 bytes)\n",
            "‚úÖ Saved: adapter_model.safetensors (159967880 bytes)\n",
            "‚úÖ Saved: special_tokens_map.json (582 bytes)\n",
            "‚úÖ Saved: tokenizer.json (3896154 bytes)\n",
            "‚úÖ Saved: tokenizer.model (499723 bytes)\n",
            "‚úÖ Saved: tokenizer_config.json (1838 bytes)\n",
            "\n",
            "üéØ Model files uploaded to: /content/model/unsloth_model\n",
            "\n",
            "üìã Files in model directory:\n",
            "  - adapter_config.json (841 bytes)\n",
            "  - adapter_model.safetensors (159,967,880 bytes)\n",
            "  - special_tokens_map.json (582 bytes)\n",
            "  - tokenizer.json (3,896,154 bytes)\n",
            "  - tokenizer.model (499,723 bytes)\n",
            "  - tokenizer_config.json (1,838 bytes)\n",
            "\n",
            "‚úÖ Found adapter_config.json - LoRA model detected\n",
            "üìä Base model: unsloth/llama-2-7b-chat-bnb-4bit\n",
            "‚úÖ Found adapter_model.safetensors\n",
            "\n",
            "üöÄ Ready! Now run the main application script to use your custom model.\n",
            "\n",
            "üìÅ ALTERNATIVE UPLOAD METHOD - Google Drive:\n",
            "\n",
            "If your model files are in Google Drive, run this instead:\n",
            "\n",
            "```python\n",
            "from google.colab import drive\n",
            "import shutil\n",
            "\n",
            "# Mount Google Drive\n",
            "drive.mount('/content/drive')\n",
            "\n",
            "# Copy your model files from Drive\n",
            "# Update this path to where your model files are stored in Drive\n",
            "source_path = '/content/drive/MyDrive/your_model_folder/unsloth_model'\n",
            "destination_path = '/content/model/unsloth_model'\n",
            "\n",
            "if os.path.exists(source_path):\n",
            "    shutil.copytree(source_path, destination_path, dirs_exist_ok=True)\n",
            "    print(f\"‚úÖ Copied model from Google Drive: {source_path}\")\n",
            "    \n",
            "    # List copied files\n",
            "    for file in os.listdir(destination_path):\n",
            "        print(f\"  - {file}\")\n",
            "else:\n",
            "    print(f\"‚ùå Model folder not found in Google Drive: {source_path}\")\n",
            "    print(\"Please update the source_path to match your Drive folder structure\")\n",
            "```\n",
            "\n",
            "\n",
            "==================================================\n",
            "MODEL VERIFICATION:\n",
            "‚úÖ LoRA adapter model detected\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# GET YOUR MODEL WORKING WITH HUGGING FACE AUTHENTICATION\n",
        "# This will properly load your LoRA on the correct Llama-2 base model\n",
        "# =============================================================================\n",
        "\n",
        "print(\"üîë SETTING UP HUGGING FACE AUTHENTICATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\"\"\n",
        "üéØ YOUR LORA NEEDS LLAMA-2 BASE MODEL!\n",
        "\n",
        "The issue: Your LoRA was trained on Llama-2 architecture, but we can't access it without authentication.\n",
        "\n",
        "SOLUTION: Get authenticated access to Llama-2 models\n",
        "\n",
        "üìã STEP-BY-STEP INSTRUCTIONS:\n",
        "\n",
        "1. üåê Go to: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "2. üìù Create a Hugging Face account (if you don't have one)\n",
        "3. ‚úÖ Request access to Llama-2 (usually approved quickly)\n",
        "4. üîë Create an access token:\n",
        "   - Go to: https://huggingface.co/settings/tokens\n",
        "   - Click \"New token\"\n",
        "   - Choose \"Read\" permissions\n",
        "   - Copy the token\n",
        "\n",
        "5. üîê Add token to Colab:\n",
        "   - Click the key icon (üîë) in the left sidebar\n",
        "   - Add new secret: Name = \"HF_TOKEN\", Value = your token\n",
        "   - Enable notebook access\n",
        "\n",
        "OR run this code with your token:\n",
        "\"\"\")\n",
        "\n",
        "# Provide authentication options\n",
        "print(\"üîë AUTHENTICATION OPTIONS:\")\n",
        "print(\"\\nOption 1 - Add HF_TOKEN to Colab secrets (RECOMMENDED)\")\n",
        "print(\"Option 2 - Login directly (run the code below)\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Try to get token from secrets first\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    login(hf_token)\n",
        "    print(\"‚úÖ Successfully authenticated with Colab secrets!\")\n",
        "    authenticated = True\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è No HF_TOKEN in secrets: {e}\")\n",
        "    authenticated = False\n",
        "\n",
        "if not authenticated:\n",
        "    print(\"\\nüîë MANUAL LOGIN:\")\n",
        "    print(\"Run this code and enter your Hugging Face token when prompted:\")\n",
        "    print(\"from huggingface_hub import login\")\n",
        "    print(\"login()\")\n",
        "\n",
        "    # Try interactive login\n",
        "    try:\n",
        "        login()\n",
        "        authenticated = True\n",
        "        print(\"‚úÖ Authentication successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Authentication failed: {e}\")\n",
        "        authenticated = False\n",
        "\n",
        "if not authenticated:\n",
        "    print(\"\"\"\n",
        "    ‚ùå AUTHENTICATION REQUIRED!\n",
        "\n",
        "    Your LoRA adapter MUST be used with Llama-2 base model.\n",
        "    Without authentication, we can only use incompatible models.\n",
        "\n",
        "    Please:\n",
        "    1. Get Llama-2 access at: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "    2. Create HF token at: https://huggingface.co/settings/tokens\n",
        "    3. Add token to Colab secrets as \"HF_TOKEN\"\n",
        "    4. Restart and run this script again\n",
        "\n",
        "    Your model configuration shows it needs:\n",
        "    - Base: unsloth/llama-2-7b-chat-bnb-4bit\n",
        "    - Target modules: ['down_proj', 'v_proj', 'q_proj', 'gate_proj', 'k_proj', 'up_proj', 'o_proj']\n",
        "\n",
        "    These are Llama-2 specific layers that don't exist in other models!\n",
        "    \"\"\")\n",
        "\n",
        "    # Provide fallback demo\n",
        "    print(\"\\nüîÑ FALLBACK OPTION:\")\n",
        "    print(\"I can create a DEMO version that simulates your model's responses...\")\n",
        "\n",
        "    create_demo = input(\"Create demo version? (y/n): \").lower().strip()\n",
        "\n",
        "    if create_demo == 'y':\n",
        "        print(\"üé® Creating demo interface...\")\n",
        "\n",
        "        # Demo version\n",
        "        import gradio as gr\n",
        "        import random\n",
        "\n",
        "        def demo_analysis(dream_text):\n",
        "            if not dream_text.strip():\n",
        "                return \"Please enter a dream to analyze.\"\n",
        "\n",
        "            # Simulate your model's analysis style\n",
        "            analyses = [\n",
        "                f\"From a Freudian perspective, your dream about '{dream_text[:30]}...' reveals significant unconscious symbolism. The imagery suggests repressed desires seeking expression through the unconscious mind. According to psychoanalytic theory, dreams are the 'royal road to the unconscious' and often contain disguised fulfillments of suppressed wishes.\",\n",
        "\n",
        "                f\"This dream contains rich psychoanalytic material. Freud would interpret the elements in '{dream_text[:30]}...' as symbolic representations of deeper psychological conflicts. The manifest content (what you remember) conceals latent content (hidden meanings) through mechanisms of displacement and condensation.\",\n",
        "\n",
        "                f\"Your dream imagery reflects classic Freudian concepts. The narrative in '{dream_text[:30]}...' likely represents wish fulfillment and the unconscious mind's attempt to process repressed material. The symbolic nature of dream elements often relates to infantile desires and unresolved psychological tensions.\"\n",
        "            ]\n",
        "\n",
        "            analysis = random.choice(analyses)\n",
        "\n",
        "            return f\"\"\"\n",
        "            <div style='padding: 20px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 15px; color: white; margin: 10px 0;'>\n",
        "                <h2 style='margin-top: 0; color: #fff;'>üß† Demo Freudian Analysis</h2>\n",
        "                <div style='background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; line-height: 1.6;'>\n",
        "                    {analysis}\n",
        "                </div>\n",
        "                <p style='margin-bottom: 0; font-size: 0.9em; opacity: 0.8;'>\n",
        "                    ‚ö†Ô∏è DEMO MODE - Not your actual model ‚Ä¢ Authenticate for real analysis\n",
        "                </p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "        # Create demo interface\n",
        "        with gr.Blocks(title=\"üß† Demo Freudian Analyzer\") as demo_interface:\n",
        "\n",
        "            gr.HTML(\"\"\"\n",
        "            <div style='text-align: center; padding: 20px; background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); border-radius: 15px; color: white; margin-bottom: 20px;'>\n",
        "                <h1 style='margin: 0; font-size: 2.5em;'>üß† Demo Freudian Analyzer</h1>\n",
        "                <p style='margin: 10px 0 0 0; font-size: 1.2em; opacity: 0.9;'>\n",
        "                    Demo Mode - Authenticate to Use Your Real Model\n",
        "                </p>\n",
        "            </div>\n",
        "            \"\"\")\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### ‚ö†Ô∏è This is a DEMO version\n",
        "\n",
        "            To use **YOUR actual trained model**, you need:\n",
        "            1. Hugging Face authentication for Llama-2 access\n",
        "            2. Your 160MB LoRA adapter requires the correct base model\n",
        "\n",
        "            **Your Model Details:**\n",
        "            - Base: `unsloth/llama-2-7b-chat-bnb-4bit`\n",
        "            - LoRA: 16 rank, 32 alpha\n",
        "            - Target: Llama-2 attention layers\n",
        "            \"\"\")\n",
        "\n",
        "            dream_input = gr.Textbox(\n",
        "                lines=6,\n",
        "                placeholder=\"Enter your dream (this is demo mode - not your actual model)\",\n",
        "                label=\"üåô Dream Description\"\n",
        "            )\n",
        "\n",
        "            analyze_btn = gr.Button(\"üîç Demo Analysis\", variant=\"secondary\")\n",
        "            analysis_output = gr.HTML()\n",
        "\n",
        "            analyze_btn.click(\n",
        "                fn=demo_analysis,\n",
        "                inputs=[dream_input],\n",
        "                outputs=[analysis_output]\n",
        "            )\n",
        "\n",
        "        # Launch demo\n",
        "        demo_interface.launch(share=True, debug=False)\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Cannot proceed without authentication.\")\n",
        "\n",
        "else:\n",
        "    # AUTHENTICATED - Load your real model!\n",
        "    print(\"üéâ AUTHENTICATION SUCCESS! Loading YOUR real model...\")\n",
        "\n",
        "    import torch\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "    from peft import PeftModel\n",
        "    import gradio as gr\n",
        "\n",
        "    # Clear GPU memory\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"üî• GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "    class AuthenticatedModelLoader:\n",
        "        def __init__(self):\n",
        "            self.model = None\n",
        "            self.tokenizer = None\n",
        "            self.model_loaded = False\n",
        "            self.load_error = None\n",
        "\n",
        "            try:\n",
        "                self._load_authenticated_model()\n",
        "            except Exception as e:\n",
        "                self.load_error = str(e)\n",
        "                print(f\"‚ùå Model loading failed: {e}\")\n",
        "\n",
        "        def _load_authenticated_model(self):\n",
        "            \"\"\"Load your model with proper authentication\"\"\"\n",
        "\n",
        "            model_path = \"/content/model/unsloth_model\"\n",
        "            base_model = \"meta-llama/Llama-2-7b-chat-hf\"  # Standard Llama-2\n",
        "\n",
        "            print(f\"üéØ Loading YOUR authenticated model:\")\n",
        "            print(f\"   Base: {base_model}\")\n",
        "            print(f\"   LoRA: {model_path}\")\n",
        "\n",
        "            # Load tokenizer\n",
        "            print(\"üìù Loading Llama-2 tokenizer...\")\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            print(\"‚úÖ Tokenizer loaded\")\n",
        "\n",
        "            # Load base model\n",
        "            print(\"ü§ñ Loading Llama-2 base model...\")\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                base_model,\n",
        "                device_map=\"auto\",\n",
        "                torch_dtype=torch.float16,\n",
        "                low_cpu_mem_usage=True,\n",
        "                trust_remote_code=True\n",
        "            )\n",
        "            print(\"‚úÖ Base model loaded\")\n",
        "\n",
        "            # Load YOUR LoRA\n",
        "            print(\"üîó Loading YOUR LoRA adapter...\")\n",
        "            self.model = PeftModel.from_pretrained(self.model, model_path)\n",
        "            self.model.eval()\n",
        "            print(\"‚úÖ YOUR LoRA loaded successfully!\")\n",
        "\n",
        "            self.model_loaded = True\n",
        "            print(\"üéâ YOUR CUSTOM MODEL IS FULLY LOADED!\")\n",
        "\n",
        "        def analyze_dream(self, dream_text):\n",
        "            if not self.model_loaded:\n",
        "                return f\"‚ùå Model failed to load: {self.load_error}\"\n",
        "\n",
        "            if not dream_text.strip():\n",
        "                return \"Please enter a dream to analyze.\"\n",
        "\n",
        "            # Clear memory\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Your model's training format\n",
        "            system_prompt = \"You are an expert Freudian psychoanalyst. Analyze dreams using Freudian psychological concepts, symbolism, and unconscious desires.\"\n",
        "            prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\nAnalyze this dream from a Freudian perspective:\\n\\n{dream_text} [/INST]\"\n",
        "\n",
        "            try:\n",
        "                inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1200)\n",
        "                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=400,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.9,\n",
        "                        repetition_penalty=1.1,\n",
        "                        pad_token_id=self.tokenizer.eos_token_id,\n",
        "                        eos_token_id=self.tokenizer.eos_token_id\n",
        "                    )\n",
        "\n",
        "                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                analysis = response[len(prompt):].strip()\n",
        "                analysis = analysis.replace(\"[/INST]\", \"\").replace(\"</s>\", \"\").strip()\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "                return f\"\"\"\n",
        "                <div style='padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; margin: 10px 0;'>\n",
        "                    <h2 style='margin-top: 0; color: #fff;'>üß† YOUR Custom Freudian Analysis</h2>\n",
        "                    <div style='background: rgba(255,255,255,0.1); padding: 15px; border-radius: 10px; line-height: 1.6;'>\n",
        "                        {analysis}\n",
        "                    </div>\n",
        "                    <p style='margin-bottom: 0; font-size: 0.9em; opacity: 0.8;'>\n",
        "                        üéØ Generated by YOUR Custom LoRA (16/32) ‚Ä¢ Specialized for Freudian Analysis\n",
        "                    </p>\n",
        "                </div>\n",
        "                \"\"\"\n",
        "\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå Generation error: {str(e)}\"\n",
        "\n",
        "    # Create interface for your authenticated model\n",
        "    print(\"üé® Creating interface for YOUR authenticated model...\")\n",
        "\n",
        "    analyzer = AuthenticatedModelLoader()\n",
        "\n",
        "    with gr.Blocks(title=\"üß† YOUR Authenticated Model\") as interface:\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style='text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; color: white; margin-bottom: 20px;'>\n",
        "            <h1 style='margin: 0; font-size: 2.5em;'>üß† YOUR Custom Freudian Model</h1>\n",
        "            <p style='margin: 10px 0 0 0; font-size: 1.2em; opacity: 0.9;'>\n",
        "                ‚úÖ Authenticated & Fully Loaded\n",
        "            </p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "        status = \"üéØ YOUR Model Active\" if analyzer.model_loaded else \"‚ùå Loading Failed\"\n",
        "\n",
        "        gr.Markdown(f\"\"\"\n",
        "        ### üî• GPU Active ‚Ä¢ {status}\n",
        "\n",
        "        **Congratulations!** YOUR custom model is now properly loaded:\n",
        "        - ‚úÖ Authenticated access to Llama-2\n",
        "        - ‚úÖ YOUR 160MB LoRA adapter loaded\n",
        "        - ‚úÖ Correct target modules matched\n",
        "        - ‚úÖ Ready for specialized Freudian analysis\n",
        "        \"\"\")\n",
        "\n",
        "        dream_input = gr.Textbox(\n",
        "            lines=7,\n",
        "            placeholder=\"Describe your dream... YOUR custom model will provide specialized Freudian analysis.\",\n",
        "            label=\"üåô Dream Description\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            analyze_btn = gr.Button(\"üîç Analyze with YOUR Model\", variant=\"primary\", size=\"lg\")\n",
        "            clear_btn = gr.Button(\"üóëÔ∏è Clear\")\n",
        "\n",
        "        analysis_output = gr.HTML(label=\"YOUR Model's Analysis\")\n",
        "\n",
        "        # Examples\n",
        "        examples = [\n",
        "            [\"I was flying over water but my wings turned heavy and I fell toward dark waves below.\"],\n",
        "            [\"I found myself in my childhood home but all the rooms were rearranged and unfamiliar.\"],\n",
        "            [\"I was being chased through a maze by a shadowy figure I couldn't identify.\"],\n",
        "            [\"I was taking an exam but the questions were all about my personal secrets.\"],\n",
        "            [\"I was at a party where I knew everyone but realized I wasn't wearing clothes.\"]\n",
        "        ]\n",
        "\n",
        "        gr.Examples(examples, inputs=dream_input, label=\"üí≠ Test YOUR Model\")\n",
        "\n",
        "        def clear_all():\n",
        "            return \"\", \"\"\n",
        "\n",
        "        analyze_btn.click(\n",
        "            fn=analyzer.analyze_dream,\n",
        "            inputs=[dream_input],\n",
        "            outputs=[analysis_output]\n",
        "        )\n",
        "\n",
        "        clear_btn.click(\n",
        "            fn=clear_all,\n",
        "            outputs=[dream_input, analysis_output]\n",
        "        )\n",
        "\n",
        "        gr.HTML(\"\"\"\n",
        "        <div style='text-align: center; padding: 15px; margin-top: 20px; border-top: 1px solid #eee; color: #666;'>\n",
        "            <p>üéØ YOUR Custom Model ‚Ä¢ Llama-2 + 160MB LoRA ‚Ä¢ Freudian Specialist</p>\n",
        "        </div>\n",
        "        \"\"\")\n",
        "\n",
        "    # Launch your authenticated model\n",
        "    interface.launch(share=True, debug=False)\n",
        "    print(\"üéâ YOUR AUTHENTICATED MODEL IS NOW RUNNING!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"SUMMARY:\")\n",
        "if authenticated:\n",
        "    print(\"‚úÖ SUCCESS: Your model should now be working with proper Llama-2 base!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è DEMO: Authenticate with Hugging Face to use your real model\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5749f8f111f342749688d6c7c986fabb",
            "441bdc0d416c409d9ad91100d099376e",
            "bfc4c7623dc4419bb780d815e07c13c8",
            "751011c9d5e14948822b2b944981bb19",
            "8ec2efa546f64ac79f26846b1e2e5042",
            "32729d7b85d748529bc442707303168c",
            "f81fb17539204566836269f2fc18a2f4",
            "6ece17e736ab4090b0deac69e0f33129",
            "43c8b0542be54ed49a2c7ea9e776b8c7",
            "7147527ec6c34c33838c45b01c046b46",
            "a5aca34a7c0a4f0eaaed471f4b1f95ff",
            "570bab9c9a654bf5b457a7f212a3ed1c",
            "24ad8f6c61164dd8889bf141e6dfe740",
            "c86c924fae4949abb54cb650574957ae",
            "d223a16e590b4d62a019024195b56e8b",
            "31a16fab339e494f929c14af8b63fe83",
            "9d45453b58a8498b84fee0c2efc45258",
            "4d30211241b740ff87d473bc4a72f989",
            "1b879e86e4e443f4bf9b71c0884b822a",
            "7a3181ff1eb74f3e91972bb59359c887"
          ]
        },
        "id": "KeFEp9e5WI4J",
        "outputId": "b087e767-25d0-4450-ee12-c9528566d849"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë SETTING UP HUGGING FACE AUTHENTICATION\n",
            "==================================================\n",
            "\n",
            "üéØ YOUR LORA NEEDS LLAMA-2 BASE MODEL!\n",
            "\n",
            "The issue: Your LoRA was trained on Llama-2 architecture, but we can't access it without authentication.\n",
            "\n",
            "SOLUTION: Get authenticated access to Llama-2 models\n",
            "\n",
            "üìã STEP-BY-STEP INSTRUCTIONS:\n",
            "\n",
            "1. üåê Go to: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
            "2. üìù Create a Hugging Face account (if you don't have one)\n",
            "3. ‚úÖ Request access to Llama-2 (usually approved quickly)\n",
            "4. üîë Create an access token:\n",
            "   - Go to: https://huggingface.co/settings/tokens\n",
            "   - Click \"New token\"\n",
            "   - Choose \"Read\" permissions\n",
            "   - Copy the token\n",
            "\n",
            "5. üîê Add token to Colab:\n",
            "   - Click the key icon (üîë) in the left sidebar\n",
            "   - Add new secret: Name = \"HF_TOKEN\", Value = your token\n",
            "   - Enable notebook access\n",
            "\n",
            "OR run this code with your token:\n",
            "\n",
            "üîë AUTHENTICATION OPTIONS:\n",
            "\n",
            "Option 1 - Add HF_TOKEN to Colab secrets (RECOMMENDED)\n",
            "Option 2 - Login directly (run the code below)\n",
            "‚ö†Ô∏è No HF_TOKEN in secrets: Secret HF_TOKEN does not exist.\n",
            "\n",
            "üîë MANUAL LOGIN:\n",
            "Run this code and enter your Hugging Face token when prompted:\n",
            "from huggingface_hub import login\n",
            "login()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5749f8f111f342749688d6c7c986fabb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Authentication successful!\n",
            "üéâ AUTHENTICATION SUCCESS! Loading YOUR real model...\n",
            "üî• GPU: Tesla T4\n",
            "üé® Creating interface for YOUR authenticated model...\n",
            "üéØ Loading YOUR authenticated model:\n",
            "   Base: meta-llama/Llama-2-7b-chat-hf\n",
            "   LoRA: /content/model/unsloth_model\n",
            "üìù Loading Llama-2 tokenizer...\n",
            "‚ùå Model loading failed: You are trying to access a gated repo.\n",
            "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
            "401 Client Error. (Request ID: Root=1-683c85d9-489a5429488131f21dffcbc4;5a9e655d-1230-45e2-9747-48ce2a770bd3)\n",
            "\n",
            "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
            "Access to model meta-llama/Llama-2-7b-chat-hf is restricted. You must have access to it and be authenticated to access it. Please log in.\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://2ce0b0cc6664d58f6b.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2ce0b0cc6664d58f6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'gradio' has no attribute 'blocks'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0ea951936af6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;31m# Launch your authenticated model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üéâ YOUR AUTHENTICATED MODEL IS NOW RUNNING!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, _frontend, enable_monitoring)\u001b[0m\n\u001b[1;32m   2575\u001b[0m                         \u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2576\u001b[0m                     )\n\u001b[0;32m-> 2577\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2578\u001b[0m                     \u001b[0martifact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJavascript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2579\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/analytics.py\u001b[0m in \u001b[0;36mlaunched_analytics\u001b[0;34m(blocks, data)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mcore_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_block_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcore_gradio_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     additional_data = {\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/utils.py\u001b[0m in \u001b[0;36mcore_gradio_components\u001b[0;34m()\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ChatInterface\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Interface\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Blocks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TabbedInterface\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"NativePlot\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     ]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gradio/utils.py\u001b[0m in \u001b[0;36mget_all_components\u001b[0;34m()\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gradio' has no attribute 'blocks'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5749f8f111f342749688d6c7c986fabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_f81fb17539204566836269f2fc18a2f4"
          }
        },
        "441bdc0d416c409d9ad91100d099376e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ece17e736ab4090b0deac69e0f33129",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_43c8b0542be54ed49a2c7ea9e776b8c7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "bfc4c7623dc4419bb780d815e07c13c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7147527ec6c34c33838c45b01c046b46",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a5aca34a7c0a4f0eaaed471f4b1f95ff",
            "value": ""
          }
        },
        "751011c9d5e14948822b2b944981bb19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_570bab9c9a654bf5b457a7f212a3ed1c",
            "style": "IPY_MODEL_24ad8f6c61164dd8889bf141e6dfe740",
            "value": true
          }
        },
        "8ec2efa546f64ac79f26846b1e2e5042": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_c86c924fae4949abb54cb650574957ae",
            "style": "IPY_MODEL_d223a16e590b4d62a019024195b56e8b",
            "tooltip": ""
          }
        },
        "32729d7b85d748529bc442707303168c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31a16fab339e494f929c14af8b63fe83",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9d45453b58a8498b84fee0c2efc45258",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "f81fb17539204566836269f2fc18a2f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "6ece17e736ab4090b0deac69e0f33129": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c8b0542be54ed49a2c7ea9e776b8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7147527ec6c34c33838c45b01c046b46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5aca34a7c0a4f0eaaed471f4b1f95ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "570bab9c9a654bf5b457a7f212a3ed1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24ad8f6c61164dd8889bf141e6dfe740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c86c924fae4949abb54cb650574957ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d223a16e590b4d62a019024195b56e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "31a16fab339e494f929c14af8b63fe83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d45453b58a8498b84fee0c2efc45258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d30211241b740ff87d473bc4a72f989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b879e86e4e443f4bf9b71c0884b822a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7a3181ff1eb74f3e91972bb59359c887",
            "value": "Connecting..."
          }
        },
        "1b879e86e4e443f4bf9b71c0884b822a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a3181ff1eb74f3e91972bb59359c887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}